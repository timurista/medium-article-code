{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Redefining Context in Transformers: The Role of Neural Attention Memory Models\n",
                "\n",
                "This notebook demonstrates the key concepts and implementation of Neural Attention Memory Models (NAMMs) in transformer architectures. We'll explore how NAMMs improve context management and memory handling in transformer models through practical examples and visualizations."
            ]
        },
        {
            "cell_type": "markdown", 
            "metadata": {},
            "source": [
                "## Setup\n",
                "First, let's import the required libraries:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "tf.random.set_seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Traditional Transformer Context Management\n",
                "\n", 
                "Let's implement the basic scaled dot-product attention mechanism used in traditional transformers:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scaled_dot_product_attention(q, k, v, mask=None):\n",
                "    \"\"\"Calculate the attention weights.\n",
                "    q, k, v must have matching leading dimensions.\n",
                "    \"\"\"\n",
                "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
                "    \n",
                "    # Scale matmul_qk\n",
                "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
                "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
                "\n",
                "    # Add the mask if provided\n",
                "    if mask is not None:\n",
                "        scaled_attention_logits += (mask * -1e9)\n",
                "\n",
                "    # Softmax is normalized on the last axis (seq_len_k)\n",
                "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
                "    output = tf.matmul(attention_weights, v)\n",
                "\n",
                "    return output, attention_weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Neural Attention Memory Model (NAMM) Implementation\n",
                "\n",
                "Now let's implement a basic NAMM layer that extends the traditional attention mechanism with memory management:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NAMM(tf.keras.layers.Layer):\n",
                "    def __init__(self, memory_size, embedding_dim, **kwargs):\n",
                "        super(NAMM, self).__init__(**kwargs)\n",
                "        self.memory_size = memory_size\n",
                "        self.embedding_dim = embedding_dim\n",
                "        \n",
                "        # Initialize memory\n",
                "        self.memory = tf.Variable(\n",
                "            initial_value=tf.random.normal((1, memory_size, embedding_dim)),\n",
                "            trainable=True,\n",
                "            name='memory'\n",
                "        )\n",
                "        \n",
                "    def call(self, inputs):\n",
                "        # Combine input with memory\n",
                "        batch_size = tf.shape(inputs)[0]\n",
                "        memory_batch = tf.tile(self.memory, [batch_size, 1, 1])\n",
                "        combined = tf.concat([memory_batch, inputs], axis=1)\n",
                "        \n",
                "        # Apply attention\n",
                "        output, attention_weights = scaled_dot_product_attention(\n",
                "            combined, combined, combined\n",
                "        )\n",
                "        \n",
                "        # Update memory\n",
                "        new_memory = output[:, :self.memory_size, :]\n",
                "        self.memory.assign(tf.reduce_mean(new_memory, axis=0, keepdims=True))\n",
                "        \n",
                "        return output[:, self.memory_size:, :]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python", 
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}