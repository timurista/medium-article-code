{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Future of Localized AI: Cost, Accessibility, and Customization",
                "\n",
                "This notebook demonstrates key concepts and implementations related to running AI models locally using tools like Ollama and Swarm. We'll explore cost benefits, accessibility improvements, and customization capabilities of local AI deployments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setting up Local AI Environment",
                "\n",
                "Below we demonstrate how to set up and initialize a local LLM using the Hugging Face transformers library. This example uses a smaller model for demonstration purposes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def initialize_local_model(model_name=\"gpt2\"):\n",
                "    try:\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
                "        return model, tokenizer\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading model: {e}\")\n",
                "        return None, None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cost Analysis Visualization",
                "\n",
                "Let's create a visualization comparing the costs of local AI deployment versus cloud-based solutions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample cost data\n",
                "months = np.arange(1, 13)\n",
                "cloud_costs = months * 300  # $300 per month\n",
                "local_costs = 1000 + months * 50  # $1000 initial + $50 per month\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(months, cloud_costs, label='Cloud-based API')\n",
                "plt.plot(months, local_costs, label='Local Deployment')\n",
                "plt.xlabel('Months')\n",
                "plt.ylabel('Cumulative Cost ($)')\n",
                "plt.title('Cost Comparison: Local vs Cloud AI Deployment')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Best Practices for Local AI Deployment",
                "\n",
                "1. Always implement proper error handling\n",
                "2. Monitor system resources (RAM, CPU usage)\n",
                "3. Implement proper model versioning\n",
                "4. Cache results when possible\n",
                "5. Use appropriate batch sizes for your hardware"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LocalAIManager:\n",
                "    def __init__(self, model_name):\n",
                "        self.model, self.tokenizer = initialize_local_model(model_name)\n",
                "        self.cache = {}\n",
                "    \n",
                "    def generate_text(self, prompt, max_length=50):\n",
                "        try:\n",
                "            # Check cache first\n",
                "            if prompt in self.cache:\n",
                "                return self.cache[prompt]\n",
                "            \n",
                "            # Generate new response\n",
                "            inputs = self.tokenizer(prompt, return_tensors='pt')\n",
                "            outputs = self.model.generate(**inputs, max_length=max_length)\n",
                "            response = self.tokenizer.decode(outputs[0])\n",
                "            \n",
                "            # Cache the result\n",
                "            self.cache[prompt] = response\n",
                "            return response\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error generating text: {e}\")\n",
                "            return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion",
                "\n",
                "This notebook has demonstrated key concepts in local AI deployment including:\n",
                "- Setting up local models\n",
                "- Cost analysis and visualization\n",
                "- Best practices implementation\n",
                "- Error handling and resource management\n",
                "\n",
                "Local AI deployment offers significant advantages in terms of cost, privacy, and customization, making it an attractive option for many applications."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}