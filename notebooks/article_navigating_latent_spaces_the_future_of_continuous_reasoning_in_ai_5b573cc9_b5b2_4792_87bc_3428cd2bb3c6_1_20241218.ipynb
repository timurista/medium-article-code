{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata":": {},
            "source": [
                "# Navigating Latent Spaces: The Future of Continuous Reasoning in AI",
                "",
                "This notebook explores the concepts of latent spaces in AI, their role in continuous reasoning, and practical applications using Python code examples."
            ]
        },
        {
            "cell_type": "markdown", 
            "metadata": {},
            "source": [
                "## Setup and Required Libraries",
                "",
                "First, let's import the necessary libraries for our examples:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import numpy as np",
                "import pandas as pd",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns",
                "from sklearn.manifold import TSNE",
                "from sklearn.preprocessing import StandardScaler",
                "import torch",
                "import torch.nn as nn",
                "",
                "# Set random seeds for reproducibility",
                "np.random.seed(42)",
                "torch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Understanding Latent Spaces",
                "",
                "Latent spaces are continuous, high-dimensional spaces where data points represent meaningful features of the input data. They form the backbone of many modern AI models, especially in generative tasks and representation learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Create a simple autoencoder to demonstrate latent space",
                "class SimpleAutoencoder(nn.Module):",
                "    def __init__(self, input_dim, latent_dim):",
                "        super().__init__()",
                "        self.encoder = nn.Sequential(",
                "            nn.Linear(input_dim, 128),",
                "            nn.ReLU(),",
                "            nn.Linear(128, latent_dim)",
                "        )",
                "        self.decoder = nn.Sequential(",
                "            nn.Linear(latent_dim, 128),",
                "            nn.ReLU(),",
                "            nn.Linear(128, input_dim)",
                "        )",
                "",
                "    def forward(self, x):",
                "        latent = self.encoder(x)",
                "        reconstructed = self.decoder(latent)",
                "        return reconstructed, latent"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Visualizing Latent Spaces",
                "",
                "Let's create a visualization of a simple latent space using synthetic data:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Generate synthetic data",
                "n_samples = 1000",
                "data = np.random.randn(n_samples, 10)",
                "",
                "# Create and train autoencoder",
                "model = SimpleAutoencoder(10, 2)",
                "data_tensor = torch.FloatTensor(data)",
                "",
                "# Plot latent space",
                "with torch.no_grad():",
                "    _, latent = model(data_tensor)",
                "    latent = latent.numpy()",
                "",
                "plt.figure(figsize=(10, 8))",
                "plt.scatter(latent[:, 0], latent[:, 1], alpha=0.5)",
                "plt.title('2D Latent Space Visualization')",
                "plt.xlabel('Latent Dimension 1')",
                "plt.ylabel('Latent Dimension 2')",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Error Handling and Best Practices",
                "",
                "When working with latent spaces, it's important to handle errors and edge cases properly:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def safe_encode(model, data, device='cpu'):",
                "    \"\"\"Safely encode data into latent space with error handling\"\"\"",
                "    try:",
                "        if not torch.is_tensor(data):",
                "            data = torch.FloatTensor(data)",
                "        data = data.to(device)",
                "        ",
                "        with torch.no_grad():",
                "            _, latent = model(data)",
                "        return latent.cpu().numpy()",
                "        ",
                "    except RuntimeError as e:",
                "        print(f'Error during encoding: {e}')",
                "        return None",
                "    except Exception as e:",
                "        print(f'Unexpected error: {e}')",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion",
                "",
                "This notebook demonstrated key concepts in working with latent spaces for AI applications. We covered:",
                "",
                "- Basic concepts of latent spaces",
                "- Implementation of a simple autoencoder",
                "- Visualization techniques",
                "- Error handling and best practices",
                "",
                "For more advanced applications, consider exploring variational autoencoders (VAEs) and other sophisticated architectures."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}