{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Capabilities: Understanding the Emergent Properties of Transformers",
                "",
                "## Introduction",
                "Emergent properties in large language models (LLMs) are crucial for understanding how these AI systems operate and interact. This notebook explores the transformer architecture, its emergent capabilities, and practical implementations.",
                "",
                "We'll cover:",
                "- Transformer architecture fundamentals",
                "- Implementation of key components",
                "- Visualization of attention mechanisms", 
                "- Practical examples and best practices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Import required libraries\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown", 
            "metadata": {},
            "source": [
                "## Self-Attention Implementation",
                "",
                "Let's implement the core self-attention mechanism of transformers. This is the fundamental building block that enables contextual understanding in LLMs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class SelfAttention(nn.Module):\n",
                "    def __init__(self, embed_size, heads):\n",
                "        super(SelfAttention, self).__init__()\n",
                "        self.embed_size = embed_size\n",
                "        self.heads = heads\n",
                "        self.head_dim = embed_size // heads\n",
                "\n",
                "        assert (self.head_dim * heads == embed_size), \"Embedding size needs to be divisible by heads\"\n",
                "\n",
                "        # Linear transformations for Q, K, V\n",
                "        self.values = nn.Linear(embed_size, embed_size)\n",
                "        self.keys = nn.Linear(embed_size, embed_size)\n",
                "        self.queries = nn.Linear(embed_size, embed_size)\n",
                "        \n",
                "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
                "\n",
                "    def forward(self, values, keys, query):\n",
                "        N = query.shape[0]\n",
                "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
                "\n",
                "        # Split into heads\n",
                "        values = self.values(values).reshape(N, value_len, self.heads, self.head_dim)\n",
                "        keys = self.keys(keys).reshape(N, key_len, self.heads, self.head_dim)\n",
                "        queries = self.queries(query).reshape(N, query_len, self.heads, self.head_dim)\n",
                "\n",
                "        # Calculate attention scores\n",
                "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
                "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
                "        \n",
                "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
                "            N, query_len, self.heads * self.head_dim\n",
                "        )\n",
                "\n",
                "        return self.fc_out(out)"
            ],
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python", 
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}