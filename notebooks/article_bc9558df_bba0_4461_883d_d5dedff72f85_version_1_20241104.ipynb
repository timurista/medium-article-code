{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Scaling AI Model Deployments with LitServe: A Technical Guide\n",
                "\n",
                "This notebook demonstrates how to implement and scale AI model deployments using LitServe and FastAPI. We'll cover architecture, implementation, optimization, and deployment strategies.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Installation\n",
                "\n", 
                "First, let's install the required packages:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Install required packages\n",
                "!pip install litserve fastapi uvicorn transformers torch"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown", 
            "metadata": {},
            "source": [
                "## Basic LitServe Implementation\n",
                "\n",
                "Let's create a simple text generation API using LitServe and a pretrained model:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from litserve import LitServe\n",
                "from transformers import pipeline\n",
                "import asyncio\n",
                "\n",
                "# Initialize LitServe app\n",
                "app = LitServe()\n",
                "\n",
                "# Load pretrained model\n",
                "model = pipeline(\"text-generation\", model=\"gpt2\")\n",
                "\n",
                "@app.post(\"/generate-text/\")\n",
                "async def generate_text(prompt: str):\n",
                "    \"\"\"Generate text based on input prompt\"\"\"\n",
                "    result = model(prompt)\n",
                "    return {\"generated_text\": result}"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing Batching\n",
                "\n",
                "Here's how to implement batched inference for better performance:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from typing import List\n",
                "\n",
                "@app.post(\"/batch-predict/\")\n",
                "async def batch_predict(prompts: List[str]):\n",
                "    \"\"\"Process multiple prompts in a single batch\"\"\"\n",
                "    # Configure batch size\n",
                "    batch_size = min(len(prompts), 16)\n",
                "    \n",
                "    # Process in batches\n",
                "    results = []\n",
                "    for i in range(0, len(prompts), batch_size):\n",
                "        batch = prompts[i:i + batch_size]\n",
                "        batch_results = model(batch)\n",
                "        results.extend(batch_results)\n",
                "        \n",
                "    return {\"predictions\": results}"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Monitoring and Logging\n",
                "\n",
                "Implementing basic monitoring and logging functionality:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import logging\n",
                "import time\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "@app.post(\"/monitored-predict/\")\n",
                "async def monitored_predict(prompt: str):\n",
                "    \"\"\"Endpoint with monitoring and logging\"\"\"\n",
                "    start_time = time.time()\n",
                "    \n",
                "    try:\n",
                "        logger.info(f\"Processing request with prompt: {prompt}\")\n",
                "        result = model(prompt)\n",
                "        \n",
                "        processing_time = time.time() - start_time\n",
                "        logger.info(f\"Request processed in {processing_time:.2f} seconds\")\n",
                "        \n",
                "        return {\n",
                "            \"result\": result,\n",
                "            \"processing_time\": processing_time\n",
                "        }\n",
                "        \n",
                "    except Exception as e:\n",
                "        logger.error(f\"Error processing request: {str(e)}\")\n",
                "        raise"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing the Implementation\n",
                "\n",
                "Let's test our endpoints with some sample requests:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import requests\n",
                "\n",
                "# Test single prediction\n",
                "response = requests.post(\n",
                "    \"http://localhost:8000/generate-text/\",\n",
                "    json={\"prompt\": \"Once upon a time\"}\n",
                ")\n",
                "print(\"Single prediction result:\", response.json())\n",
                "\n",
                "# Test batch prediction\n",
                "batch_response = requests.post(\n",
                "    \"http://localhost:8000/batch-predict/\",\n",
                "    json={\"prompts\": [\"Hello\", \"World\", \"Test\"]}\n",
                ")\n",
                "print(\"Batch prediction results:\", batch_response.json())"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "This notebook demonstrated the key concepts of using LitServe for AI model deployment, including:\n",
                "- Basic setup and implementation\n",
                "- Batching for improved performance\n",
                "- Monitoring and logging\n",
                "- Testing and validation\n",
                "\n",
                "For production deployments, consider additional aspects like:\n",
                "- GPU optimization\n",
                "- Error handling\n",
                "- Load balancing\n",
                "- Security measures"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}