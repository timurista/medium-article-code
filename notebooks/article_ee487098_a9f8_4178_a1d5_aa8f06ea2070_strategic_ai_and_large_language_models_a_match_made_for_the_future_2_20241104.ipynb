{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Strategic AI and Large Language Models: A Match Made for the Future?",
                "\n",
                "This notebook demonstrates key concepts and implementations related to Large Language Models (LLMs) and Strategic AI systems. We'll explore the technical foundations, practical applications, and integration approaches through code examples and visualizations."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Dependencies",
                "\nFirst, let's import the required libraries:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Understanding Large Language Models",
                "\n",
                "Large Language Models (LLMs) are built on transformer architectures that use self-attention mechanisms. Below we'll implement a basic transformer block to demonstrate the core concepts."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class TransformerBlock(nn.Module):\n    def __init__(self, embed_size, heads):\n        super(TransformerBlock, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_size, embed_size * 4),\n            nn.ReLU(),\n            nn.Linear(embed_size * 4, embed_size)\n        )\n        self.ln1 = nn.LayerNorm(embed_size)\n        self.ln2 = nn.LayerNorm(embed_size)\n        \n    def forward(self, x):\n        # Multi-head attention\n        attention = self.attention(x, x, x)[0]\n        # First residual connection and layer norm\n        x = self.ln1(attention + x)\n        # Feed forward network\n        feed_forward = self.ff(x)\n        # Second residual connection and layer norm\n        return self.ln2(feed_forward + x)\n\n# Example usage\nembedding_size = 512\nnum_heads = 8\ntransformer = TransformerBlock(embedding_size, num_heads)\n\n# Create sample input\nsample_input = torch.randn(10, 32, embedding_size)  # (sequence_length, batch_size, embedding_dim)\noutput = transformer(sample_input)\nprint(f\"Output shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Implementing a Simple Query Response System",
                "\n",
                "Let's implement a basic query response system using a pre-trained GPT-2 model:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def initialize_model():\n    try:\n        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        return model, tokenizer\n    except Exception as e:\n        print(f\"Error initializing model: {e}\")\n        return None, None\n\ndef respond_to_query(query, model, tokenizer, max_length=100):\n    try:\n        # Encode the input query\n        input_ids = tokenizer.encode(query, return_tensors='pt')\n        \n        # Generate response\n        response_ids = model.generate(\n            input_ids,\n            max_length=max_length,\n            num_return_sequences=1,\n            no_repeat_ngram_size=2,\n            temperature=0.7\n        )\n        \n        # Decode and return response\n        return tokenizer.decode(response_ids[0], skip_special_tokens=True)\n    except Exception as e:\n        return f\"Error generating response: {e}\"\n\n# Initialize model\nmodel, tokenizer = initialize_model()\n\n# Test the system\nif model and tokenizer:\n    test_query = \"What are the applications of LLMs in business?\"\n    response = respond_to_query(test_query, model, tokenizer)\n    print(f\"Query: {test_query}\\nResponse: {response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Analysis and Visualization Example",
                "\n",
                "Let's create a visualization of simulated strategic decision-making data:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Generate sample data\ndate_range = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\ndata = pd.DataFrame({\n    'date': date_range,\n    'decisions_made': np.random.normal(100, 15, len(date_range)),\n    'success_rate': np.random.uniform(0.7, 0.9, len(date_range))\n})\n\n# Create visualization\nplt.figure(figsize=(12, 6))\nplt.plot(data['date'], data['success_rate'], label='Success Rate')\nplt.title('Strategic Decision Success Rate Over Time')\nplt.xlabel('Date')\nplt.ylabel('Success Rate')\nplt.grid(True)\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Calculate some statistics\nprint(f\"Average success rate: {data['success_rate'].mean():.2f}\")\nprint(f\"Standard deviation: {data['success_rate'].std():.2f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}