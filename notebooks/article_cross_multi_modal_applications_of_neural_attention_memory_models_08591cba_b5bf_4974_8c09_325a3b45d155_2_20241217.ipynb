{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Cross-Multi-Modal Applications of Neural Attention Memory Models",
                "\nThis notebook demonstrates the implementation and concepts of Neural Attention Memory Models (NAMMs) in multi-modal AI applications."
            ]
        },
        {
            "cell_type": "markdown", 
            "metadata": {},
            "source": [
                "## Setup and Required Libraries",
                "\nFirst, let's import the necessary packages and set up our environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Multi-Head Attention Implementation",
                "\nThe core component of NAMMs is the multi-head attention mechanism. Below we implement a basic version."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_size, heads):\n        super(MultiHeadAttention, self).__init__()\n        self.heads = heads\n        self.embed_size = embed_size\n        self.head_dim = embed_size // heads\n        \n        # Validate dimensions\n        assert (self.head_dim * heads == embed_size), \"Embedding size must be divisible by heads\"\n        \n        # Initialize layers\n        self.values = nn.Linear(embed_size, embed_size, bias=False)\n        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n        self.fc_out = nn.Linear(embed_size, embed_size)\n        \n    def forward(self, x):\n        try:\n            N = x.shape[0]  # Batch size\n            length = x.shape[1]  # Sequence length\n            \n            # Split embeddings into multiple heads\n            queries = self.queries(x).view(N, length, self.heads, self.head_dim)\n            keys = self.keys(x).view(N, length, self.heads, self.head_dim)\n            values = self.values(x).view(N, length, self.heads, self.head_dim)\n            \n            # Attention calculation\n            energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n            attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n            \n            return self.fc_out(out.reshape(N, length, self.heads * self.head_dim))\n            \n        except Exception as e:\n            print(f\"Error in attention calculation: {str(e)}\")\n            raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing the Attention Model",
                "\nLet's create a simple example to test our implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create test data\nbatch_size = 64\nseq_length = 10\nembed_size = 256\nheads = 8\n\n# Initialize model and input\nmodel = MultiHeadAttention(embed_size=embed_size, heads=heads)\nsample_input = torch.rand(batch_size, seq_length, embed_size)\n\n# Forward pass\noutput = model(sample_input)\n\n# Visualize attention weights\nwith torch.no_grad():\n    attention_weights = model.forward(sample_input)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(attention_weights[0, :, :].numpy())\n    plt.title('Attention Weights Visualization')\n    plt.xlabel('Key dimension')\n    plt.ylabel('Query dimension')\n    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Best Practices and Tips",
                "\n1. Always validate input dimensions and shapes\n2. Use error handling for robust production code\n3. Implement proper initialization for attention weights\n4. Monitor attention patterns during training\n5. Consider using gradient clipping for stability"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion",
                "\nThis notebook demonstrated the implementation of Neural Attention Memory Models with a focus on the multi-head attention mechanism. We covered:\n- Basic architecture implementation\n- Attention visualization\n- Error handling\n- Best practices\n\nFor production use, consider additional optimizations and more comprehensive error handling."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python", 
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}